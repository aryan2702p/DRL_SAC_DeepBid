{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if a GPU is available and use it, otherwise use the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE = \"Solar_PBE\" \n",
    "address = \"../data/\"\n",
    "\n",
    "# data_train_csv1 = pd.read_csv(address+RE+'_16.csv', index_col=0)\n",
    "# data_train_csv2 = pd.read_csv(address+RE+'_17.csv', index_col=0)\n",
    "# data_train_csv  = pd.concat([data_train_csv1, data_train_csv2])\n",
    "# data_val_csv    = pd.read_csv(address+RE+'_18.csv', index_col=0)\n",
    "# data_test_csv   = pd.read_csv(address+RE+'_19.csv', index_col=0)\n",
    "\n",
    "# data_price = pd.read_csv(address+'Price_Elia_Imbalance_16_19.csv', index_col=0)\n",
    "# data_train_csv['Price(€)'] = data_price['Positive imbalance price'][:len(data_train_csv)]\n",
    "# data_val_csv['Price(€)']   = data_price['Positive imbalance price'][len(data_train_csv):len(data_train_csv)+len(data_val_csv)]\n",
    "# data_test_csv['Price(€)']  = data_price['Positive imbalance price'][len(data_train_csv)+len(data_val_csv):]\n",
    "\n",
    "\n",
    "data_train_csv1 = pd.read_csv(address+RE+'_16.csv', index_col=0)\n",
    "data_train_csv2 = pd.read_csv(address+RE+'_17.csv', index_col=0)\n",
    "data_train_csv  = pd.concat([data_train_csv1, data_train_csv2])\n",
    "data_val_csv    = pd.read_csv(address+RE+'_18.csv', index_col=0)\n",
    "data_test_csv   = pd.read_csv(address+RE+'_19.csv', index_col=0)\n",
    "\n",
    "\n",
    "data_price = pd.read_csv(address+'Price_Elia_Imbalance_16_19.csv', index_col=0)\n",
    "\n",
    "# Reset index to default integer indices for proper alignment\n",
    "data_price_reset = data_price.reset_index(drop=True)\n",
    "# print(data_price_reset.head(5))   \n",
    "data_train_csv = data_train_csv.reset_index(drop=True)\n",
    "data_val_csv = data_val_csv.reset_index(drop=True)\n",
    "data_test_csv = data_test_csv.reset_index(drop=True)\n",
    "\n",
    "# Now assign the values with the indices properly aligned\n",
    "data_train_csv['Price(€)'] = data_price_reset['Positive imbalance price'][:len(data_train_csv)]\n",
    "\n",
    "val_price = data_price_reset['Positive imbalance price'][len(data_train_csv):len(data_train_csv) + len(data_val_csv)]\n",
    "val_price = val_price.reset_index(drop=True)\n",
    "data_val_csv['Price(€)'] =val_price\n",
    "test_price = data_price_reset['Positive imbalance price'][len(data_train_csv) + len(data_val_csv):] \n",
    "test_price = test_price.reset_index(drop=True)\n",
    "# print(test_price.head(10))\n",
    "data_test_csv['Price(€)'] = test_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Battery_Size = 0.15  # p.u.  # Define the battery size as 0.15 per unit (p.u.)\n",
    "unit = 1  # Set the time unit to 1 (could represent 15 minutes, etc.)\n",
    "\n",
    "# Calculate the maximum renewable energy capacity from the training, validation, and test datasets\n",
    "RE_Capacity1 = max(data_train_csv['Power(MW)'])  # Max capacity in training data\n",
    "RE_Capacity2 = max(data_val_csv['Power(MW)'])    # Max capacity in validation data\n",
    "RE_Capacity3 = max(data_test_csv['Power(MW)'])   # Max capacity in test data\n",
    "\n",
    "# Get the maximum price from the price dataset\n",
    "max_price = max(data_price_reset['Marginal incremental price'])  # Max price for normalizing price data\n",
    "\n",
    "# Determine the number of units in each dataset\n",
    "size_train0 = len(data_train_csv) // unit  # Number of units in training data\n",
    "size_val0 = len(data_val_csv) // unit      # Number of units in validation data\n",
    "size_test0 = len(data_test_csv) // unit    # Number of units in test data\n",
    "\n",
    "# Function to normalize power and price data\n",
    "def normalize_data(power_data, price_data, capacity, max_price, size):\n",
    "    normalized_power = []  # Initialize list for normalized power data\n",
    "    normalized_price = []   # Initialize list for normalized price data\n",
    "    \n",
    "    # Loop through each time unit\n",
    "    for i in range(size):\n",
    "        # Calculate the average power and price for the current time unit and normalize\n",
    "        power_avg = pd.Series.mean(power_data[i * unit: (i + 1) * unit]) / capacity  # Normalized power\n",
    "        price_avg = pd.Series.mean(price_data[i * unit: (i + 1) * unit]) / max_price  # Normalized price\n",
    "        \n",
    "        # Round the normalized values to 3 decimal places\n",
    "        power_avg, price_avg = round(power_avg, 3), round(price_avg, 3)\n",
    "        \n",
    "        # Only append positive normalized power values to the list\n",
    "        if power_avg > 0:\n",
    "            normalized_power.append(power_avg)  # Add to normalized power list\n",
    "            normalized_price.append(price_avg)    # Add to normalized price list\n",
    "            \n",
    "    return normalized_power, normalized_price  # Return normalized power and price lists\n",
    "\n",
    "# Normalize the training, validation, and test datasets using the defined function\n",
    "data_train, price_train = normalize_data(data_train_csv['Power(MW)'], data_train_csv['Price(€)'], RE_Capacity1, max_price, size_train0)\n",
    "data_val, price_val = normalize_data(data_val_csv['Power(MW)'], data_val_csv['Price(€)'], RE_Capacity2, max_price, size_val0)\n",
    "data_test, price_test = normalize_data(data_test_csv['Power(MW)'], data_test_csv['Price(€)'], RE_Capacity3, max_price, size_test0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "\n",
    "BUFFER_SIZE = int(1e6)  # Replay buffer size\n",
    "BATCH_SIZE = 256        # Mini-batch size\n",
    "GAMMA = 0.99            # Discount factor\n",
    "TAU = 0.005             # For updating target networks\n",
    "LR_ACTOR = 3e-4         # Learning rate for actor\n",
    "LR_CRITIC = 3e-4        # Learning rate for critic\n",
    "ALPHA = 0.2             # Entropy regularization coefficient\n",
    "HIDDEN_SIZE = 256       # Size of hidden layers in networks\n",
    "\n",
    "# Replay Buffer to store experience tuples\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        return (torch.FloatTensor(states), torch.FloatTensor(actions),\n",
    "                torch.FloatTensor(rewards), torch.FloatTensor(next_states),\n",
    "                torch.FloatTensor(dones))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=256):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.mu = nn.Linear(hidden_size, action_dim)\n",
    "        self.log_std = nn.Linear(hidden_size, action_dim)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        mu = self.mu(x)\n",
    "        log_std = self.log_std(x).clamp(-20, 2)\n",
    "        return mu, log_std\n",
    "\n",
    "    def sample_action(self, state):\n",
    "        mu, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        z = mu + std * torch.randn_like(mu)\n",
    "        action = torch.tanh(z)\n",
    "        return action, z, mu, log_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=256):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.q = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.q(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACAgent:\n",
    "    def __init__(self, state_dim, action_dim, gamma=0.99, tau=0.005, lr=3e-4, alpha=0.2):\n",
    "        self.actor = Actor(state_dim, action_dim).to(device)\n",
    "        self.critic_1 = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_2 = Critic(state_dim, action_dim).to(device)\n",
    "        self.target_critic_1 = Critic(state_dim, action_dim).to(device)\n",
    "        self.target_critic_2 = Critic(state_dim, action_dim).to(device)\n",
    "        \n",
    "        self.target_critic_1.load_state_dict(self.critic_1.state_dict())\n",
    "        self.target_critic_2.load_state_dict(self.critic_2.state_dict())\n",
    "        \n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.critic_1_optimizer = optim.Adam(self.critic_1.parameters(), lr=lr)\n",
    "        self.critic_2_optimizer = optim.Adam(self.critic_2.parameters(), lr=lr)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        action, _, _, _ = self.actor.sample_action(state)\n",
    "        return action.detach().cpu().numpy()\n",
    "\n",
    "    def update(self, batch):\n",
    "        state, action, reward, next_state, done = batch\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        action = torch.FloatTensor(action).to(device)\n",
    "        reward = torch.FloatTensor(reward).to(device).unsqueeze(1)\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        done = torch.FloatTensor(done).to(device).unsqueeze(1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_action, _, _, log_std = self.actor.sample_action(next_state)\n",
    "            target_q1 = self.target_critic_1(next_state, next_action)\n",
    "            target_q2 = self.target_critic_2(next_state, next_action)\n",
    "            target_q = reward + (1 - done) * self.gamma * (torch.min(target_q1, target_q2) - self.alpha * log_std.exp())\n",
    "        \n",
    "        current_q1 = self.critic_1(state, action)\n",
    "        current_q2 = self.critic_2(state, action)\n",
    "        \n",
    "        critic_1_loss = torch.mean((current_q1 - target_q).pow(2))\n",
    "        critic_2_loss = torch.mean((current_q2 - target_q).pow(2))\n",
    "        \n",
    "        self.critic_1_optimizer.zero_grad()\n",
    "        critic_1_loss.backward()\n",
    "        self.critic_1_optimizer.step()\n",
    "        \n",
    "        self.critic_2_optimizer.zero_grad()\n",
    "        critic_2_loss.backward()\n",
    "        self.critic_2_optimizer.step()\n",
    "        \n",
    "        new_action, _, _, log_std = self.actor.sample_action(state)\n",
    "        q1_new = self.critic_1(state, new_action)\n",
    "        q2_new = self.critic_2(state, new_action)\n",
    "        q_new = torch.min(q1_new, q2_new)\n",
    "        \n",
    "        actor_loss = torch.mean((self.alpha * log_std.exp() - q_new))\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        for param, target_param in zip(self.critic_1.parameters(), self.target_critic_1.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        \n",
    "        for param, target_param in zip(self.critic_2.parameters(), self.target_critic_2.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment parameters\n",
    "E_max   = Battery_Size  # Maximum energy capacity of the battery\n",
    "P_max   = E_max         # Maximum power output, equal to the maximum energy capacity\n",
    "tdelta  = unit / 4      # Time step (e.g., 15 minutes if unit is in hours)\n",
    "soc_min = 0.1          # Minimum state of charge (SOC) for the battery\n",
    "soc_max = 0.9          # Maximum state of charge (SOC) for the battery\n",
    "\n",
    "# Coefficients for the battery performance and cost equations\n",
    "a0 = -1.031; a1 = 35; a2 = 3.685; a3 = 0.2156; a4 = 0.1178; a5 = 0.3201\n",
    "b0 = 0.1463; b1 = 30.27; b2 = 0.1037; b3 = 0.0584; b4 = 0.1747; b5 = 0.1288\n",
    "c0 = 0.1063; c1 = 62.49; c2 = 0.0437; d0 = 0.0712; d1 = 61.4; d2 = 0.0288\n",
    "\n",
    "# Total number of units or capacity in the system (adjust based on configuration)\n",
    "N = 130 * 215 * E_max / 0.1\n",
    "beta = 10 / max_price  # A scaling factor based on the maximum price\n",
    "\n",
    "class Env():\n",
    "    def __init__(self, data):\n",
    "        self.data_gen = data[0]  # Data for generation\n",
    "        self.data_imb = data[1]   # Data for imbalance prices\n",
    "        self.state = []            # Initialize the state of the environment\n",
    " \n",
    "    def reset(self):\n",
    "        # Reset the environment to the initial state\n",
    "        gen = self.data_gen[0]   # Get the first generation value\n",
    "        imb = self.data_imb[0]   # Get the first imbalance price\n",
    "        E = E_max / 2            # Initialize the energy state to half the maximum capacity\n",
    "        state = [[gen, imb, E]]  # Initialize the state with generation, imbalance, and energy\n",
    "        self.state = state        # Update the state of the environment\n",
    "        return state              # Return the initial state\n",
    " \n",
    "    def step(self, action):\n",
    "        index = len(self.state) % len(self.data_gen)\n",
    "        # Execute a step in the environment based on the given action\n",
    "        gen = self.data_gen[index]   # Get current generation value based on the state length\n",
    "        bid = action[0]                         # Bid amount from the action\n",
    "        rat = action[1]                         # Rate from the action\n",
    "        imb = self.data_imb[index]   # Get current imbalance price based on the state length\n",
    "\n",
    "        E = self.state[-1][-1]  # Get the current energy level from the state\n",
    "        soc = E / E_max         # Calculate state of charge (SOC)\n",
    "\n",
    "        # Calculate various parameters based on SOC\n",
    "        Voc = a0 * np.exp(-a1 * soc) + a2 + a3 * soc - a4 * soc**2 + a5 * soc**3  # Open-circuit voltage\n",
    "        Rs = b0 * np.exp(-b1 * soc) + b2 + b3 * soc - b4 * soc**2 + b5 * soc**3  # Series resistance\n",
    "        Rts = c0 * np.exp(-c1 * soc) + c2  # Total resistance in the system\n",
    "        Rtl = d0 * np.exp(-d1 * soc) + d2  # Total leakage resistance\n",
    "        R = Rs + Rts + Rtl  # Combined resistance\n",
    "\n",
    "        # Calculate maximum charging and discharging current and power\n",
    "        I_cmax = 1000000 * (E_max * soc_max - E) / N / (Voc * tdelta)\n",
    "        I_dmax = 1000000 * (E - E_max * soc_min) / N / (Voc * tdelta)\n",
    "        p_cmax = N * (Voc * I_cmax + I_cmax**2 * R)  # Maximum charging power\n",
    "        p_dmax = N * (Voc * I_dmax - I_dmax**2 * R)  # Maximum discharging power\n",
    "\n",
    "        P_cmax = p_cmax / 1000000  # Convert power to MW\n",
    "        P_dmax = p_dmax / 1000000  # Convert power to MW\n",
    "\n",
    "        # Calculate actual charging and discharging power based on bid and generation\n",
    "        P_c = min(max(rat * (gen - bid), 0), P_max, P_cmax)  # Charging power\n",
    "        P_d = min(max(rat * (bid - gen), 0), P_max, P_dmax)  # Discharging power\n",
    "\n",
    "        # Calculate currents based on charging and discharging power\n",
    "        p_c = 1000000 * P_c / N  # Convert to proper scale\n",
    "        p_d = 1000000 * P_d / N  # Convert to proper scale\n",
    "\n",
    "        # Calculate charging and discharging currents using voltage and resistance\n",
    "        I_c = -(Voc - np.sqrt(Voc**2 + 4 * R * p_c)) / (2 * R)  # Charging current\n",
    "        I_d = (Voc - np.sqrt(Voc**2 - 4 * R * p_d)) / (2 * R)    # Discharging current\n",
    "        \n",
    "        # Update the energy state based on charging/discharging\n",
    "        if not np.isclose(p_c, 0):  # If charging power is not zero\n",
    "            eff_c = (Voc * I_c) / p_c  # Calculate charging efficiency\n",
    "            eff_d = 1                   # Assume discharging efficiency is 1\n",
    "            E_prime = E + eff_c * P_c * tdelta  # Update energy state after charging\n",
    "            disp = gen - P_c            # Calculate dispatched generation\n",
    "        elif not np.isclose(p_d, 0):  # If discharging power is not zero\n",
    "            eff_d = p_d / (Voc * I_d)  # Calculate discharging efficiency\n",
    "            eff_c = 1                   # Assume charging efficiency is 1\n",
    "            E_prime = E - (1 / eff_d) * P_d * tdelta  # Update energy state after discharging\n",
    "            disp = gen + P_d            # Calculate dispatched generation\n",
    "        else:  # If neither charging nor discharging\n",
    "            eff_c = 1; eff_d = 1  # Assume efficiencies are 1\n",
    "            E_prime = E            # Energy state remains unchanged\n",
    "            disp = gen             # Dispatch generation remains the same\n",
    "\n",
    "        # Calculate revenue based on imbalance, dispatched generation, and costs\n",
    "        revenue = (imb * disp - imb * abs(bid - disp) - beta * (P_c + P_d)) * tdelta\n",
    "\n",
    "        # Update the next state with current generation, imbalance, and new energy level\n",
    "        next_state = self.state + [[gen, imb, E_prime]]\n",
    "        reward = revenue - imb * gen * tdelta  # Calculate reward\n",
    "        done = False  # Environment is not done yet\n",
    "        info = [gen, bid, rat, disp, revenue]  # Additional information for debugging\n",
    " \n",
    "        self.state = next_state  # Update the state of the environment\n",
    "        return next_state, reward, done, info  # Return the next state, reward, done flag, and info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34631\n",
      "34631\n",
      "episode: 1\n",
      "MAE_train: 38.6%         MBE_train: 39.22%        REV_train: $-82.531      ------------------------------------------------------------------------------------------\n",
      "episode: 2\n",
      "MAE_train: 22.33%        MBE_train: 21.23%        REV_train: $-22.03       ------------------------------------------------------------------------------------------\n",
      "episode: 3\n",
      "MAE_train: 24.46%        MBE_train: 23.7%         REV_train: $-45.93       ------------------------------------------------------------------------------------------\n",
      "episode: 4\n",
      "MAE_train: 24.27%        MBE_train: 23.75%        REV_train: $-47.702      ------------------------------------------------------------------------------------------\n",
      "episode: 5\n",
      "MAE_train: 23.85%        MBE_train: 23.49%        REV_train: $-44.398      ------------------------------------------------------------------------------------------\n",
      "episode: 6\n",
      "MAE_train: 22.84%        MBE_train: 22.51%        REV_train: $-36.564      ------------------------------------------------------------------------------------------\n",
      "episode: 7\n",
      "MAE_train: 21.83%        MBE_train: 21.52%        REV_train: $-29.195      ------------------------------------------------------------------------------------------\n",
      "episode: 8\n",
      "MAE_train: 21.7%         MBE_train: 21.4%         REV_train: $-28.68       ------------------------------------------------------------------------------------------\n",
      "episode: 9\n",
      "MAE_train: 21.17%        MBE_train: 20.93%        REV_train: $-22.038      ------------------------------------------------------------------------------------------\n",
      "episode: 10\n",
      "MAE_train: 21.06%        MBE_train: 20.82%        REV_train: $-21.382      ------------------------------------------------------------------------------------------\n",
      "episode: 11\n",
      "MAE_train: 21.14%        MBE_train: 20.91%        REV_train: $-21.662      ------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 85\u001b[0m\n\u001b[0;32m     82\u001b[0m         agent\u001b[38;5;241m.\u001b[39mupdate(batch)\n\u001b[0;32m     83\u001b[0m         iteration_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset counter after update\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Print metrics every 'print_interval' episodes\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize environment and agent\n",
    "state_dim = 3  # e.g., [gen, imb, E] from your environment\n",
    "action_dim = 2  # e.g., [bid, rat]\n",
    "agent = SACAgent(state_dim, action_dim)\n",
    "print(len(data_train))\n",
    "print(len(price_train))\n",
    "env_train = Env([data_train, price_train])\n",
    "# Training parameters\n",
    "num_episodes = 500\n",
    "batch_size =128\n",
    "print_interval=1\n",
    "max_buffer_size = 5000\n",
    "replay_buffer = []  # Use a replay buffer to store experiences\n",
    "max_iteration = len(data_train)  # Maximum number of iterations \n",
    "# Initialize lists to store bids, ratios, and revenue metrics for training, validation, and testing\n",
    "bid_train, bid_val, bid_test = [], [], []\n",
    "rat_train, rat_val, rat_test = [], [], []\n",
    "mae_train, mae_val, mae_test = [], [], []\n",
    "mbe_train, mbe_val, mbe_test = [], [], []\n",
    "rev_train, rev_val, rev_test = [], [], []\n",
    "\n",
    "# Define the frequency of updates\n",
    "update_frequency = 500 # Update every 10 iterations, adjust as needed\n",
    "\n",
    "for n_epi in range(num_episodes):\n",
    "    # Initialize lists for the current episode\n",
    "    bid_train.append([]); bid_val.append([]); bid_test.append([])\n",
    "    rat_train.append([]); rat_val.append([]); rat_test.append([])\n",
    "    mae_train.append([]); mae_val.append([]); mae_test.append([])\n",
    "    mbe_train.append([]); mbe_val.append([]); mbe_test.append([])\n",
    "    rev_train.append([]); rev_val.append([]); rev_test.append([])\n",
    "\n",
    "    # Reset the training environment to start a new episode\n",
    "    state = env_train.reset()\n",
    "    episode_reward = 0\n",
    "    iteration_counter = 0  # Counter for iterations since last update\n",
    "\n",
    "    for t in range(max_iteration):\n",
    "        action = agent.select_action(state[-1])\n",
    "\n",
    "        # Verify action dimension\n",
    "        assert len(action) == 2, f\"Action dimension mismatch: expected 2, got {len(action)}\"\n",
    "\n",
    "        next_state, reward, done, info = env_train.step(action)\n",
    "\n",
    "        # Verify next_state dimension\n",
    "        assert len(next_state[-1]) == state_dim, f\"Next state dimension mismatch: expected {state_dim}, got {len(next_state[-1])}\"\n",
    "\n",
    "        # Verify reward is a scalar\n",
    "        assert np.isscalar(reward), f\"Reward is not a scalar: got {type(reward)}\"\n",
    "\n",
    "        # Verify done is a boolean\n",
    "        assert isinstance(done, bool), f\"Done is not a boolean: got {type(done)}\"\n",
    "\n",
    "        # Verify info is a list with expected length (5 in this case)\n",
    "        assert isinstance(info, list) and len(info) == 5, f\"Info dimension mismatch: expected list of length 5, got {type(info)} with length {len(info)}\"\n",
    "\n",
    "        replay_buffer.append((state[-1], action, reward, next_state[-1], done))\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Unpack information from the environment step\n",
    "        gen = info[0]; bid = info[1]; rat = info[2]; disp = info[3]; revenue = info[4]\n",
    "        # Collect training data\n",
    "        bid_train[n_epi].append(bid)\n",
    "        rat_train[n_epi].append(rat)\n",
    "        mae_train[n_epi].append(abs(gen - bid))\n",
    "        mbe_train[n_epi].append(abs(disp - bid))\n",
    "        rev_train[n_epi].append(revenue)\n",
    "\n",
    "        # Increment the iteration counter\n",
    "        iteration_counter += 1\n",
    "\n",
    "        if len(replay_buffer) > max_buffer_size:\n",
    "            replay_buffer.pop(0)\n",
    "\n",
    "\n",
    "        # Update the agent if there are enough experiences and the counter reaches the update frequency\n",
    "        if len(replay_buffer) > batch_size and iteration_counter >= update_frequency:\n",
    "           # print(\"Updating batch at iteration =\", t)\n",
    "            batch = zip(*[replay_buffer[i] for i in np.random.choice(len(replay_buffer), batch_size, replace=False)])\n",
    "            agent.update(batch)\n",
    "            iteration_counter = 0  # Reset counter after update\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "    # Print metrics every 'print_interval' episodes\n",
    "    if (n_epi + 1) % print_interval == 0:\n",
    "        MAE_train = round(100 * np.mean(mae_train[n_epi]), 2)  # Mean Absolute Error for training\n",
    "        MAE_val = round(100 * np.mean(mae_val[n_epi]), 2)    # Mean Absolute Error for validation\n",
    "        MAE_test = round(100 * np.mean(mae_test[n_epi]), 2)   # Mean Absolute Error for testing\n",
    "        MBE_train = round(100 * np.mean(mbe_train[n_epi]), 2)   # Mean Bias Error for training\n",
    "        MBE_val = round(100 * np.mean(mbe_val[n_epi]), 2)     # Mean Bias Error for validation\n",
    "        MBE_test = round(100 * np.mean(mbe_test[n_epi]), 2)    # Mean Bias Error for testing\n",
    "        REV_train = round(max_price * RE_Capacity1 * np.mean(rev_train[n_epi]), 3)  # Revenue for training\n",
    "        REV_val = round(max_price * RE_Capacity2 * np.mean(rev_val[n_epi]), 3)    # Revenue for validation\n",
    "        REV_test = round(max_price * RE_Capacity3 * np.mean(rev_test[n_epi]), 3)   # Revenue for testing\n",
    "\n",
    "        # Print the results for the current episode\n",
    "        print(\"episode: {}\".format(n_epi + 1))\n",
    "        print(\"MAE_train: {}%\".format(MAE_train).ljust(25), end=\"\")\n",
    "       # print(\"MAE_val: {}%\".format(MAE_val).ljust(25), end=\"\")\n",
    "       # print(\"MAE_test: {}%\".format(MAE_test).ljust(25))\n",
    "        print(\"MBE_train: {}%\".format(MBE_train).ljust(25), end=\"\")\n",
    "       # print(\"MBE_val: {}%\".format(MBE_val).ljust(25), end=\"\")\n",
    "       # print(\"MBE_test: {}%\".format(MBE_test).ljust(25))\n",
    "        print(\"REV_train: ${}\".format(REV_train).ljust(25), end=\"\")\n",
    "      #  print(\"REV_val: ${}\".format(REV_val).ljust(25), end=\"\")\n",
    "      #  print(\"REV_test: ${}\".format(REV_test).ljust(25))\n",
    "        print(\"------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
